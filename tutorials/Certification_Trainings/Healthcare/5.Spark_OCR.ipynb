{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "5.Spark_OCR.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I08sFJYCxR0Z"
      },
      "source": [
        "![JohnSnowLabs](https://nlp.johnsnowlabs.com/assets/images/logo.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VJbAaitZrXus"
      },
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/JohnSnowLabs/spark-nlp-workshop/blob/master/tutorials/Certification_Trainings/Healthcare/5.Spark_OCR.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Niy3mZAjoayg"
      },
      "source": [
        "# Spark OCR "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "okhT7AcXxben"
      },
      "source": [
        "## Colab Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w2SLMn7K23vQ",
        "outputId": "44fb4bac-1d90-476d-a893-1c1ca5f796f4",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 73
        }
      },
      "source": [
        "import json\n",
        "\n",
        "from google.colab import files\n",
        "\n",
        "license_keys = files.upload()\n",
        "\n",
        "with open(list(license_keys.keys())[0]) as f:\n",
        "    license_keys = json.load(f)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-f086a406-909d-4179-a7b9-a33f32cccac8\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-f086a406-909d-4179-a7b9-a33f32cccac8\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving spark_ocr.json to spark_ocr (1).json\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0sWxFfdYLQI8"
      },
      "source": [
        "%%capture\n",
        "for k,v in license_keys.items(): \n",
        "    %set_env $k=$v\n",
        "\n",
        "!wget https://raw.githubusercontent.com/JohnSnowLabs/spark-nlp-workshop/master/jsl_colab_setup.sh\n",
        "!bash jsl_colab_setup.sh"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2uaNGU8QNxVj",
        "outputId": "29105858-c5bd-4cb1-88f9-3d02f18883d7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import os \n",
        "\n",
        "os.environ['JSL_OCR_LICENSE'] = license_keys['SPARK_OCR_LICENSE']\n",
        "ocr_secret = license_keys['JSL_OCR_SECRET'] \n",
        "ocr_version = license_keys['JSL_OCR_SECRET'].split('-')[0]\n",
        "\n",
        "! pip install spark-nlp-display\n",
        "! pip install spark-ocr==$ocr_version'.spark30' --user --extra-index-url=https://pypi.johnsnowlabs.com/$ocr_secret --upgrade\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: spark-nlp-display in /usr/local/lib/python3.7/dist-packages (1.7)\n",
            "Requirement already satisfied: svgwrite==1.4 in /usr/local/lib/python3.7/dist-packages (from spark-nlp-display) (1.4)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from spark-nlp-display) (1.1.5)\n",
            "Requirement already satisfied: spark-nlp in /usr/local/lib/python3.7/dist-packages (from spark-nlp-display) (3.0.1)\n",
            "Requirement already satisfied: numpy in /root/.local/lib/python3.7/site-packages (from spark-nlp-display) (1.17.4)\n",
            "Requirement already satisfied: ipython in /usr/local/lib/python3.7/dist-packages (from spark-nlp-display) (5.5.0)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->spark-nlp-display) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->spark-nlp-display) (2.8.1)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from ipython->spark-nlp-display) (4.4.2)\n",
            "Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.4 in /usr/local/lib/python3.7/dist-packages (from ipython->spark-nlp-display) (1.0.18)\n",
            "Requirement already satisfied: simplegeneric>0.8 in /usr/local/lib/python3.7/dist-packages (from ipython->spark-nlp-display) (0.8.1)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.7/dist-packages (from ipython->spark-nlp-display) (57.0.0)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.7/dist-packages (from ipython->spark-nlp-display) (0.7.5)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.7/dist-packages (from ipython->spark-nlp-display) (5.0.5)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.7/dist-packages (from ipython->spark-nlp-display) (2.6.1)\n",
            "Requirement already satisfied: pexpect; sys_platform != \"win32\" in /usr/local/lib/python3.7/dist-packages (from ipython->spark-nlp-display) (4.8.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->spark-nlp-display) (1.15.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython->spark-nlp-display) (0.2.5)\n",
            "Requirement already satisfied: ipython-genutils in /usr/local/lib/python3.7/dist-packages (from traitlets>=4.2->ipython->spark-nlp-display) (0.2.0)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.7/dist-packages (from pexpect; sys_platform != \"win32\"->ipython->spark-nlp-display) (0.7.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://pypi.johnsnowlabs.com/3.1.0-55698a0f8a192e7bc4fbb409020c6406205a9911\n",
            "Requirement already up-to-date: spark-ocr==3.1.0.spark30 in /root/.local/lib/python3.7/site-packages (3.1.0.spark30)\n",
            "Requirement already satisfied, skipping upgrade: py4j==0.10.9 in /usr/local/lib/python3.7/dist-packages (from spark-ocr==3.1.0.spark30) (0.10.9)\n",
            "Processing /root/.cache/pip/wheels/5e/bd/07/031766ca628adec8435bb40f0bd83bb676ce65ff4007f8e73f/pyspark-3.0.1-py2.py3-none-any.whl\n",
            "Requirement already satisfied, skipping upgrade: scikit-image==0.16.2 in /usr/local/lib/python3.7/dist-packages (from spark-ocr==3.1.0.spark30) (0.16.2)\n",
            "Requirement already satisfied, skipping upgrade: pillow==6.2.1 in /root/.local/lib/python3.7/site-packages (from spark-ocr==3.1.0.spark30) (6.2.1)\n",
            "Requirement already satisfied, skipping upgrade: implicits==1.0.2 in /root/.local/lib/python3.7/site-packages (from spark-ocr==3.1.0.spark30) (1.0.2)\n",
            "Requirement already satisfied, skipping upgrade: numpy==1.17.4 in /root/.local/lib/python3.7/site-packages (from spark-ocr==3.1.0.spark30) (1.17.4)\n",
            "Requirement already satisfied, skipping upgrade: scipy>=0.19.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image==0.16.2->spark-ocr==3.1.0.spark30) (1.4.1)\n",
            "Requirement already satisfied, skipping upgrade: imageio>=2.3.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image==0.16.2->spark-ocr==3.1.0.spark30) (2.4.1)\n",
            "Requirement already satisfied, skipping upgrade: networkx>=2.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image==0.16.2->spark-ocr==3.1.0.spark30) (2.5.1)\n",
            "Requirement already satisfied, skipping upgrade: matplotlib!=3.0.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image==0.16.2->spark-ocr==3.1.0.spark30) (3.2.2)\n",
            "Requirement already satisfied, skipping upgrade: PyWavelets>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image==0.16.2->spark-ocr==3.1.0.spark30) (1.1.1)\n",
            "Requirement already satisfied, skipping upgrade: decorator<5,>=4.3 in /usr/local/lib/python3.7/dist-packages (from networkx>=2.0->scikit-image==0.16.2->spark-ocr==3.1.0.spark30) (4.4.2)\n",
            "Requirement already satisfied, skipping upgrade: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image==0.16.2->spark-ocr==3.1.0.spark30) (2.8.1)\n",
            "Requirement already satisfied, skipping upgrade: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image==0.16.2->spark-ocr==3.1.0.spark30) (2.4.7)\n",
            "Requirement already satisfied, skipping upgrade: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image==0.16.2->spark-ocr==3.1.0.spark30) (0.10.0)\n",
            "Requirement already satisfied, skipping upgrade: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image==0.16.2->spark-ocr==3.1.0.spark30) (1.3.1)\n",
            "Requirement already satisfied, skipping upgrade: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib!=3.0.0,>=2.0.0->scikit-image==0.16.2->spark-ocr==3.1.0.spark30) (1.15.0)\n",
            "Installing collected packages: pyspark\n",
            "Successfully installed pyspark-3.0.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LRGeiAF1q1vv"
      },
      "source": [
        "**!!! ATTENTION : After you run this cell ^^ , you need to RESTART the COLAB RUNTIME and RE-RUN the all cells above AGAIN due to Colab predefined settings.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "id": "lC4Tt3onRBJy",
        "outputId": "8109e32b-3c7f-4ed3-c133-29c0e54c6061"
      },
      "source": [
        "import sparkocr\n",
        "import sys\n",
        "from pyspark.sql import SparkSession\n",
        "from sparkocr import start\n",
        "import os\n",
        "import base64\n",
        "from sparkocr.transformers import *\n",
        "from pyspark.ml import PipelineModel\n",
        "from pyspark.sql import functions as F\n",
        "\n",
        "spark = start(secret=license_keys['JSL_OCR_SECRET'])\n",
        "spark"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Spark version: 3.0.1\n",
            "Spark NLP version: 3.0.1\n",
            "Spark OCR version: 3.1.0\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - in-memory</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://2ad7f3294aad:4040\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.0.1</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local[*]</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>Spark OCR</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ],
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x7f43d5f7ad50>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wrcd5xhwrcC_"
      },
      "source": [
        "# Pdf to Text \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kj8tsYfljiBP"
      },
      "source": [
        "!wget -q -O sample_doc.pdf \"/content/Biochemistry B.S.cnew-rotated.pdf\"\n"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U83p8mY9huRo"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FwAGTafIj4Bi"
      },
      "source": [
        "def pipeline():\n",
        "    \n",
        "    # Transforrm PDF document to images per page\n",
        "    pdf_to_image = PdfToImage()\\\n",
        "          .setInputCol(\"content\")\\\n",
        "          .setOutputCol(\"image\")\n",
        "\n",
        "    # Run OCR\n",
        "    ocr = ImageToText()\\\n",
        "          .setInputCol(\"image\")\\\n",
        "          .setOutputCol(\"text\")\\\n",
        "          .setConfidenceThreshold(65)\n",
        "    \n",
        "    pipeline = PipelineModel(stages=[\n",
        "        pdf_to_image,\n",
        "        ocr\n",
        "    ])\n",
        "    \n",
        "    return pipeline"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xNh4i1Woj6oo"
      },
      "source": [
        "pdf = 'sample_doc.pdf'\n",
        "pdf_example_df = spark.read.format(\"binaryFile\").load(pdf).cache()"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tp7a9yMqkCGu"
      },
      "source": [
        "result = pipeline().transform(pdf_example_df).cache()"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VWND8q95kE47",
        "outputId": "8574fd6a-d946-4d36-aafc-fe0218737985"
      },
      "source": [
        "result.select(\"pagenum\",\"text\", \"confidence\").show()"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-------+----+----------+\n",
            "|pagenum|text|confidence|\n",
            "+-------+----+----------+\n",
            "+-------+----+----------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rj7rv4b7kTNo",
        "outputId": "839c93af-4da2-4e4a-e8fb-5ea284aed277"
      },
      "source": [
        "result.select(\"text\").collect()"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bqbnotCfkK58",
        "outputId": "7e61d4e6-8cc2-4654-acb2-1a6076174929"
      },
      "source": [
        "print(\"\\n\".join([row.text for row in result.select(\"text\").collect()]))\n"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kdsJk00Br8CC"
      },
      "source": [
        "###  With Skew Correction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BAXZ3PFNjmgo"
      },
      "source": [
        "from sparkocr.transformers import *\n",
        "from pyspark.ml import PipelineModel\n",
        "from sparkocr.utils import display_image\n",
        "from sparkocr.metrics import score"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6cIb7V9IsBNi"
      },
      "source": [
        "def ocr_pipeline(skew_correction=False):\n",
        "    \n",
        "    # Transforrm PDF document to images per page\n",
        "    pdf_to_image = PdfToImage()\\\n",
        "          .setInputCol(\"content\")\\\n",
        "          .setOutputCol(\"image\")\n",
        "\n",
        "    # Image skew corrector \n",
        "    skew_corrector = ImageSkewCorrector()\\\n",
        "          .setInputCol(\"image\")\\\n",
        "          .setOutputCol(\"corrected_image\")\\\n",
        "          .setAutomaticSkewCorrection(skew_correction)\n",
        "\n",
        "    # Run OCR\n",
        "    ocr = ImageToText()\\\n",
        "          .setInputCol(\"corrected_image\")\\\n",
        "          .setOutputCol(\"text\")\n",
        "    \n",
        "    pipeline = PipelineModel(stages=[\n",
        "        pdf_to_image,\n",
        "        skew_corrector,\n",
        "        ocr\n",
        "    ])\n",
        "    \n",
        "    return pipeline"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OJcxUPPqOois"
      },
      "source": [
        "!wget -q \"/content/Biochemistry B.S.cnew-rotated.pdf\""
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0UlhfHEYtU76"
      },
      "source": [
        "pdf_rotated_df = spark.read.format(\"binaryFile\").load('Biochemistry B.S.cnew-rotated.pdf').cache()"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bYQe1heJtXYx"
      },
      "source": [
        "pdf_pipeline = ocr_pipeline(False) \n",
        "\n",
        "result = pdf_pipeline.transform(pdf_rotated_df).cache()\n"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-5lY6fZnQOj4",
        "outputId": "007e753b-465c-43e0-f9e3-65ddd9412386"
      },
      "source": [
        "result.show()"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+--------------------+-------------------+--------+--------------------+-----------+-------+-----------+--------------------+------------------+---------+--------------------+--------------------+\n",
            "|                path|   modificationTime|  length|               image|total_pages|pagenum|documentnum|     corrected_image|        confidence|exception|                text|           positions|\n",
            "+--------------------+-------------------+--------+--------------------+-----------+-------+-----------+--------------------+------------------+---------+--------------------+--------------------+\n",
            "|file:/content/Bio...|2021-06-04 19:51:59|17748679|[file:/content/Bi...|        126|      0|          0|[file:/content/Bi...| 40.75541305541992|         |ev Bh\n",
            "\n",
            "eek AS opi...|[[[[ev Bh\n",
            "\n",
            ", 0, 4...|\n",
            "|file:/content/Bio...|2021-06-04 19:51:59|17748679|[file:/content/Bi...|        126|      1|          0|[file:/content/Bi...|49.388052417385964|         |wy Lph ef\n",
            "\n",
            "SE 9 p...|[[[[wy Lph ef\n",
            "\n",
            "SE...|\n",
            "|file:/content/Bio...|2021-06-04 19:51:59|17748679|[file:/content/Bi...|        126|      2|          0|[file:/content/Bi...|52.088284759521486|         |e a= wee\n",
            "\n",
            "   \n",
            "   ...|[[[[e a= wee\n",
            "\n",
            ", 0...|\n",
            "|file:/content/Bio...|2021-06-04 19:51:59|17748679|[file:/content/Bi...|        126|      3|          0|[file:/content/Bi...| 42.43988999866304|         |i er\n",
            "eo Cope 8 Fl...|[[[[i er\n",
            "eo Cope ...|\n",
            "|file:/content/Bio...|2021-06-04 19:51:59|17748679|[file:/content/Bi...|        126|      4|          0|[file:/content/Bi...| 54.57109899106233|         |i v/s. 8 alich 50...|[[[[i v/s. 8 alic...|\n",
            "|file:/content/Bio...|2021-06-04 19:51:59|17748679|[file:/content/Bi...|        126|      5|          0|[file:/content/Bi...| 58.42138481140137|         |ter FS VBaLS op S...|[[[[ter FS VBaLS ...|\n",
            "|file:/content/Bio...|2021-06-04 19:51:59|17748679|[file:/content/Bi...|        126|      6|          0|[file:/content/Bi...| 66.88379917144775|         | \n",
            "\n",
            "ih\n",
            "\n",
            "jst 5 Glew...|[[[[ \n",
            "\n",
            ", 0, 0.0, ...|\n",
            "|file:/content/Bio...|2021-06-04 19:51:59|17748679|[file:/content/Bi...|        126|      7|          0|[file:/content/Bi...| 74.48406066894532|         | \n",
            "\n",
            "H,OH\n",
            "\n",
            " \n",
            "\n",
            "     ...|[[[[ \n",
            "\n",
            ", 0, 728.0...|\n",
            "|file:/content/Bio...|2021-06-04 19:51:59|17748679|[file:/content/Bi...|        126|      8|          0|[file:/content/Bi...| 67.07714223861694|         |= wen\n",
            "\n",
            "vs} Leh at...|[[[[= wen\n",
            "\n",
            ", 0, 3...|\n",
            "|file:/content/Bio...|2021-06-04 19:51:59|17748679|[file:/content/Bi...|        126|      9|          0|[file:/content/Bi...|58.274946039373226|         | \n",
            "\n",
            " \n",
            "\n",
            "we Wl de 9 ...|[[[[ \n",
            "\n",
            ", 0, 2311....|\n",
            "|file:/content/Bio...|2021-06-04 19:51:59|17748679|[file:/content/Bi...|        126|     10|          0|[file:/content/Bi...| 65.23844564528693|         | \n",
            "\n",
            " \n",
            "\n",
            "vO yal ed >...|[[[[ \n",
            "\n",
            ", 0, 1518....|\n",
            "|file:/content/Bio...|2021-06-04 19:51:59|17748679|[file:/content/Bi...|        126|     11|          0|[file:/content/Bi...|53.243855678673945|         |Vv pl way S =\n",
            "\n",
            "ls...|[[[[Vv pl way S =...|\n",
            "|file:/content/Bio...|2021-06-04 19:51:59|17748679|[file:/content/Bi...|        126|     12|          0|[file:/content/Bi...| 53.69026844675948|         | \n",
            "\n",
            "vs Yale S ire\n",
            "...|[[[[ \n",
            "\n",
            ", 0, 204.0...|\n",
            "|file:/content/Bio...|2021-06-04 19:51:59|17748679|[file:/content/Bi...|        126|     13|          0|[file:/content/Bi...| 41.46274646492891|         |us (git yaad» $2\n",
            "...|[[[[us (git yaad»...|\n",
            "|file:/content/Bio...|2021-06-04 19:51:59|17748679|[file:/content/Bi...|        126|     14|          0|[file:/content/Bi...|63.203996620926205|         | \n",
            "\n",
            "   \n",
            "\n",
            "  \n",
            "\n",
            "td tw...|[[[[ \n",
            "\n",
            ", 0, 196.0...|\n",
            "|file:/content/Bio...|2021-06-04 19:51:59|17748679|[file:/content/Bi...|        126|     15|          0|[file:/content/Bi...| 70.96685699462891|         | \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            "ail\n",
            "\n",
            "ees...|[[[[ \n",
            "\n",
            ", 0, 8.0, ...|\n",
            "|file:/content/Bio...|2021-06-04 19:51:59|17748679|[file:/content/Bi...|        126|     16|          0|[file:/content/Bi...|55.492984570954974|         | \n",
            "  \n",
            " \n",
            "  \n",
            " \n",
            "  \n",
            "  ...|[[[[ \n",
            "  \n",
            " \n",
            "  \n",
            " \n",
            " ...|\n",
            "|file:/content/Bio...|2021-06-04 19:51:59|17748679|[file:/content/Bi...|        126|     17|          0|[file:/content/Bi...| 51.96181633893181|         | \n",
            "\n",
            "or Aol ted 9 8...|[[[[ \n",
            "\n",
            ", 0, 1230....|\n",
            "|file:/content/Bio...|2021-06-04 19:51:59|17748679|[file:/content/Bi...|        126|     18|          0|[file:/content/Bi...|62.473697662353516|         | \n",
            "\n",
            " \n",
            "\n",
            "   \n",
            "\n",
            "ao . e...|[[[[ \n",
            "\n",
            ", 0, 280.0...|\n",
            "|file:/content/Bio...|2021-06-04 19:51:59|17748679|[file:/content/Bi...|        126|     19|          0|[file:/content/Bi...|50.283811268053554|         |Ov\n",
            "\n",
            "ela\n",
            "\n",
            "Myo 3292...|[[[[Ov\n",
            "\n",
            ", 0, 174....|\n",
            "+--------------------+-------------------+--------+--------------------+-----------+-------+-----------+--------------------+------------------+---------+--------------------+--------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KIngWL7bQDcb",
        "outputId": "8094f423-44f0-40fc-daf8-886c5e1c3643"
      },
      "source": [
        "result.select(\"pagenum\").collect()[0].pagenum"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "rR57LCA5PluJ",
        "outputId": "b1215317-e0f2-4237-d840-78e78128d99a"
      },
      "source": [
        "display_image(result.select(\"image\").collect()[0].image)"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "error",
          "ename": "Py4JJavaError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-34-dc8c306e6fac>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdisplay_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"image\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/root/.local/lib/python3.7/site-packages/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mcollect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    594\u001b[0m         \"\"\"\n\u001b[1;32m    595\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mSCCallSiteSync\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcss\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 596\u001b[0;31m             \u001b[0msock_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollectToPython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    597\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msock_info\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBatchedSerializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPickleSerializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    598\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1304\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1305\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1307\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/root/.local/lib/python3.7/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    126\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    129\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n",
            "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o215.collectToPython.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 5.0 failed 1 times, most recent failure: Lost task 0.0 in stage 5.0 (TID 5, 2ad7f3294aad, executor driver): org.apache.spark.SparkException: Kryo serialization failed: Buffer overflow. Available: 75497472, required: 134217728. To avoid this, increase spark.kryoserializer.buffer.max value.\n\tat org.apache.spark.serializer.KryoSerializerInstance.serialize(KryoSerializer.scala:381)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:491)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: com.esotericsoftware.kryo.KryoException: Buffer overflow. Available: 75497472, required: 134217728\n\tat com.esotericsoftware.kryo.io.Output.require(Output.java:167)\n\tat com.esotericsoftware.kryo.io.Output.writeBytes(Output.java:251)\n\tat com.esotericsoftware.kryo.io.Output.writeBytes(Output.java:237)\n\tat com.esotericsoftware.kryo.serializers.DefaultArraySerializers$ByteArraySerializer.write(DefaultArraySerializers.java:49)\n\tat com.esotericsoftware.kryo.serializers.DefaultArraySerializers$ByteArraySerializer.write(DefaultArraySerializers.java:38)\n\tat com.esotericsoftware.kryo.Kryo.writeClassAndObject(Kryo.java:651)\n\tat com.twitter.chill.Tuple2Serializer.write(TupleSerializers.scala:37)\n\tat com.twitter.chill.Tuple2Serializer.write(TupleSerializers.scala:33)\n\tat com.esotericsoftware.kryo.Kryo.writeClassAndObject(Kryo.java:651)\n\tat com.esotericsoftware.kryo.serializers.DefaultArraySerializers$ObjectArraySerializer.write(DefaultArraySerializers.java:361)\n\tat com.esotericsoftware.kryo.serializers.DefaultArraySerializers$ObjectArraySerializer.write(DefaultArraySerializers.java:302)\n\tat com.esotericsoftware.kryo.Kryo.writeClassAndObject(Kryo.java:651)\n\tat org.apache.spark.serializer.KryoSerializerInstance.serialize(KryoSerializer.scala:377)\n\t... 4 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2059)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2008)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2007)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2007)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:973)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:973)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:973)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2239)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2188)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2177)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:775)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2099)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2120)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2139)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2164)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1004)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:388)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1003)\n\tat org.apache.spark.sql.execution.SparkPlan.executeCollect(SparkPlan.scala:385)\n\tat org.apache.spark.sql.Dataset.$anonfun$collectToPython$1(Dataset.scala:3450)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3618)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:100)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:160)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:87)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:764)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3616)\n\tat org.apache.spark.sql.Dataset.collectToPython(Dataset.scala:3447)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.SparkException: Kryo serialization failed: Buffer overflow. Available: 75497472, required: 134217728. To avoid this, increase spark.kryoserializer.buffer.max value.\n\tat org.apache.spark.serializer.KryoSerializerInstance.serialize(KryoSerializer.scala:381)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:491)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\nCaused by: com.esotericsoftware.kryo.KryoException: Buffer overflow. Available: 75497472, required: 134217728\n\tat com.esotericsoftware.kryo.io.Output.require(Output.java:167)\n\tat com.esotericsoftware.kryo.io.Output.writeBytes(Output.java:251)\n\tat com.esotericsoftware.kryo.io.Output.writeBytes(Output.java:237)\n\tat com.esotericsoftware.kryo.serializers.DefaultArraySerializers$ByteArraySerializer.write(DefaultArraySerializers.java:49)\n\tat com.esotericsoftware.kryo.serializers.DefaultArraySerializers$ByteArraySerializer.write(DefaultArraySerializers.java:38)\n\tat com.esotericsoftware.kryo.Kryo.writeClassAndObject(Kryo.java:651)\n\tat com.twitter.chill.Tuple2Serializer.write(TupleSerializers.scala:37)\n\tat com.twitter.chill.Tuple2Serializer.write(TupleSerializers.scala:33)\n\tat com.esotericsoftware.kryo.Kryo.writeClassAndObject(Kryo.java:651)\n\tat com.esotericsoftware.kryo.serializers.DefaultArraySerializers$ObjectArraySerializer.write(DefaultArraySerializers.java:361)\n\tat com.esotericsoftware.kryo.serializers.DefaultArraySerializers$ObjectArraySerializer.write(DefaultArraySerializers.java:302)\n\tat com.esotericsoftware.kryo.Kryo.writeClassAndObject(Kryo.java:651)\n\tat org.apache.spark.serializer.KryoSerializerInstance.serialize(KryoSerializer.scala:377)\n\t... 4 more\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dl-Ilq4Zwfhq"
      },
      "source": [
        "### Display recognized text without skew correction\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9EU_b2y4tavx"
      },
      "source": [
        "result.select(\"pagenum\",\"text\", \"confidence\").show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eNxCvtgqwlIC"
      },
      "source": [
        "print(\"\\n\".join([row.text for row in result.select(\"text\").collect()]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "quH_2fczw0Td"
      },
      "source": [
        "### Display results with skew correction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6SPde4WBQ-Vt"
      },
      "source": [
        "pdf_pipeline = ocr_pipeline(True) \n",
        "\n",
        "corrected_result = pdf_pipeline.transform(pdf_rotated_df).cache()\n",
        "\n",
        "print(\"\\n\".join([row.text for row in corrected_result.select(\"text\").collect()]))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vgn7YFZtw7kY"
      },
      "source": [
        "corrected_result.select(\"pagenum\",\"text\", \"confidence\").show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Ek0XS9CzBTL"
      },
      "source": [
        "### Display skew corrected images"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iKsIxz7EzCs6"
      },
      "source": [
        "display_image(corrected_result.select(\"corrected_image\").collect()[0].corrected_image)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3FyRn9dHxMU5"
      },
      "source": [
        "## Compute score and compare\n",
        "Read original text and calculate scores for both results."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qUsJW3AoRTXd"
      },
      "source": [
        "!wget -q https://raw.githubusercontent.com/JohnSnowLabs/spark-nlp-workshop/master/tutorials/Certification_Trainings/Healthcare/data/ocr/400.txt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XszOJzejwAwR"
      },
      "source": [
        "detected = \"\\n\".join([row.text for row in result.collect()])\n",
        "corrected_detected = \"\\n\".join([row.text for row in corrected_result.collect()])\n",
        "\n",
        "# read original text\n",
        "pdf_rotated_text = open('400.txt', \"r\").read()\n",
        "\n",
        "# compute scores\n",
        "detected_score = score(pdf_rotated_text, detected)\n",
        "corrected_score = score(pdf_rotated_text, corrected_detected)\n",
        "\n",
        "#  print scores\n",
        "print(\"Score without skew correction: {0}\".format(detected_score))\n",
        "print(\"Score with skew correction: {0}\".format(corrected_score))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HT2g2VF4STV8"
      },
      "source": [
        "## Reading multiple pdfs from folder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EoWSrO2E2WBg"
      },
      "source": [
        "pdf_path = \"/content/*.pdf\"\n",
        "\n",
        "pdfs = spark.read.format(\"binaryFile\").load(pdf_path).cache()\n",
        "#images = spark.read.format(\"binaryFile\").load('text_with_noise.png').cache()\n",
        "\n",
        "pdfs.count()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t1HmMgqLuxvi"
      },
      "source": [
        "# Transforrm PDF document to images per page\n",
        "pdf_to_image = PdfToImage()\\\n",
        "      .setInputCol(\"content\")\\\n",
        "      .setOutputCol(\"image\")\n",
        "\n",
        "# Run OCR\n",
        "ocr = ImageToText()\\\n",
        "      .setInputCol(\"image\")\\\n",
        "      .setOutputCol(\"text\")\\\n",
        "      .setConfidenceThreshold(65)\\\n",
        "      .setIgnoreResolution(False)\n",
        "\n",
        "ocr_pipeline = PipelineModel(stages=[\n",
        "    pdf_to_image,\n",
        "    ocr\n",
        "])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vF9M6OC_4UGU"
      },
      "source": [
        "results = ocr_pipeline.transform(pdfs)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pKfGC4D2aR1d"
      },
      "source": [
        "results.columns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HBPkZ7QeaTif"
      },
      "source": [
        "results.select('path','confidence','text').show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kXy09T8mwRJF"
      },
      "source": [
        "## Image processing after reading a pdf"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G2O7WqltKVR0"
      },
      "source": [
        "from sparkocr.enums import *\n",
        "\n",
        "# Read binary as image\n",
        "pdf_to_image = PdfToImage()\\\n",
        "  .setInputCol(\"content\")\\\n",
        "  .setOutputCol(\"image\")\\\n",
        "  .setResolution(400)\n",
        "\n",
        "# Binarize using adaptive tresholding\n",
        "binarizer = ImageAdaptiveThresholding()\\\n",
        "  .setInputCol(\"image\")\\\n",
        "  .setOutputCol(\"binarized_image\")\\\n",
        "  .setBlockSize(91)\\\n",
        "  .setOffset(50)\n",
        "\n",
        "# Apply morphology opening\n",
        "opening = ImageMorphologyOperation()\\\n",
        "  .setKernelShape(KernelShape.SQUARE)\\\n",
        "  .setOperation(MorphologyOperationType.OPENING)\\\n",
        "  .setKernelSize(3)\\\n",
        "  .setInputCol(\"binarized_image\")\\\n",
        "  .setOutputCol(\"opening_image\")\n",
        "\n",
        "# Remove small objects\n",
        "remove_objects = ImageRemoveObjects()\\\n",
        "  .setInputCol(\"opening_image\")\\\n",
        "  .setOutputCol(\"corrected_image\")\\\n",
        "  .setMinSizeObject(130)\n",
        "\n",
        "# Image Layout Analyzer for detect regions\n",
        "image_layout_analyzer = ImageLayoutAnalyzer()\\\n",
        "  .setInputCol(\"corrected_image\")\\\n",
        "  .setOutputCol(\"region\")\\\n",
        "\n",
        "draw_regions = ImageDrawRegions()\\\n",
        "  .setInputCol(\"corrected_image\")\\\n",
        "  .setInputRegionsCol(\"region\")\\\n",
        "  .setOutputCol(\"image_with_regions\")\n",
        "\n",
        "# Run tesseract OCR for corrected image\n",
        "ocr_corrected = ImageToText()\\\n",
        "  .setInputCol(\"corrected_image\")\\\n",
        "  .setOutputCol(\"corrected_text\")\\\n",
        "  .setPositionsCol(\"corrected_positions\")\\\n",
        "  .setConfidenceThreshold(65)\n",
        "\n",
        "# Run OCR for original image\n",
        "ocr = ImageToText()\\\n",
        "  .setInputCol(\"image\")\\\n",
        "  .setOutputCol(\"text\")\n",
        "\n",
        "# OCR pipeline\n",
        "image_pipeline = PipelineModel(stages=[\n",
        "    pdf_to_image,\n",
        "    binarizer,\n",
        "    opening,\n",
        "    remove_objects,\n",
        "    image_layout_analyzer,\n",
        "    draw_regions,\n",
        "    ocr,\n",
        "    ocr_corrected\n",
        "])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tM5rz4KwtBEL"
      },
      "source": [
        "! wget -q https://raw.githubusercontent.com/JohnSnowLabs/spark-ocr-workshop/master/jupyter/data/pdfs/noised.pdf\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JAIAhfJitIPZ"
      },
      "source": [
        "image_df = spark.read.format(\"binaryFile\").load('noised.pdf').cache()\n",
        "image_df.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VsHSMgLqtOo-"
      },
      "source": [
        "result = image_pipeline \\\n",
        ".transform(image_df) \\\n",
        ".cache()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wX7phZ3z0BYj"
      },
      "source": [
        "for r in result.distinct().collect():\n",
        "  \n",
        "    print(\"Original: %s\" % r.path)\n",
        "    display_image(r.image)\n",
        "\n",
        "    print(\"Corrected: %s\" % r.path)\n",
        "    display_image(r.corrected_image)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VD2BoEMwt8oZ"
      },
      "source": [
        "### Results with original image"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cY__BZUStTVw"
      },
      "source": [
        "from termcolor import colored\n",
        "\n",
        "grouped_results = result.groupBy(\"path\", \"pagenum\").agg(F.concat_ws(\"\", F.collect_list(\"text\")).alias(\"text\"))\n",
        "for row in grouped_results.collect():\n",
        "    print(colored(\"Filename:\\n%s , page: %d\" % (row.path, row.pagenum), \"red\"))\n",
        "    print(\"Recognized text:\\n%s\" % row.text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mHOpQf8YtgwI"
      },
      "source": [
        "### Results with corrected image\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QYLD5GYitUxQ"
      },
      "source": [
        "grouped_results = result.groupBy(\"path\", \"pagenum\").agg(F.concat_ws(\"\", F.collect_list(\"corrected_text\")).alias(\"corrected_text\"))\n",
        "for row in grouped_results.collect():\n",
        "    print(colored(\"Filename:\\n%s , page: %d\" % (row.path, row.pagenum), \"red\"))\n",
        "    print(\"Recognized text:\\n%s\" % row.corrected_text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KmcmM3NquFPW"
      },
      "source": [
        "result.columns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_K582Shu1dUj"
      },
      "source": [
        "### Abby output"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ecBhTMWI1cGF"
      },
      "source": [
        "abbyy = \"\"\"-----\n",
        "% Date: 7/16/68\n",
        "X*: I; * • ■ Sample No. 5031___ — .*\n",
        "•* Original request made by _____Mr. C. L. Tucker, Jr. on\n",
        "Sample specifications written by\n",
        "BLEND CASING RECASING\n",
        "OLD GOLD STRAIGHT Tobacco Blend\n",
        "Control for Sample No. 5030\n",
        "John H. M. Bohlken\n",
        "FINAL FLAVOR\n",
        ") 7/10/68\n",
        "MENTHOL FLAVOR\n",
        "• Cigarettes; * . .v\\ . /,*, *, S •\n",
        "Brand --------- OLD GOLD STRAIGHT -V . ••••\n",
        "; . L e n g t h ------- — 85 mm. . : '\n",
        "Circumference-- 25.3 mm. • ' *;. • •\n",
        "P a p e r ---------- Ecusta 556 • * .\n",
        "F i r m n e s s---- —— OLD GOLD STRAIGHT . ! •■'\n",
        "D r a w ___________ OLD GOLD STRAIGHT\n",
        "W e i g h t --------- 0LD GOLD STRAIGHT Wrappings: « -\n",
        "Tipping Paper — — *\n",
        "p H n f —. — — _ _ ~ L a b e l s ----OLD GOLD STRAIGHT\n",
        "( • Filter Length-- . — Closures--- Standard Blue .\n",
        "^ ^ ; • Tear Tape— Gold\n",
        "Cartons --- OLD GOLD STRAIGHT\n",
        "s Requirements: . - •' • Markings-- Sample number on each\n",
        "• pack and carton Laboratory----- One Tray .\n",
        "O t h e r s --------- * , s • • . 4\n",
        "Laboratory A n a l ysis^ I \" '/***• * 7 ' ^ ^\n",
        "Tars and Nicotine, Taste Panel, Burning Time, Gas Phase Analysis,\n",
        "Benzo (A) Pyrene Analyses — J-ZZ-Zf'- (£. / •\n",
        "Responsibility;\n",
        "Tobacco B l e n d ------Manufacturing - A. Kraus . . * -\n",
        "Filter Production--- —\n",
        "• Making & P a c k i n g---Product Development , John H. M. Bohlken\n",
        "Shipping -----------\n",
        "Reports:\n",
        "t\n",
        "Written by — John H. M. Bohlken\n",
        "Original to - Mr. C. L. Tucker, Jr.\n",
        "Copies t o ---Dr. A. W. Spears\n",
        "• 9 ..\n",
        "\"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9V7_s8Bs1jZi"
      },
      "source": [
        "#### Display original and corrected images with regions\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J5-kjby21fl_"
      },
      "source": [
        "for r in result.select(\"path\",\"image\",\"image_with_regions\").distinct().collect():\n",
        "    print(\"Original: %s\" % r.path)\n",
        "    display_image(r.image)\n",
        "    print(\"Corrected: %s\" % r.path)\n",
        "    display_image(r.image_with_regions)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DoR5iX2G989t"
      },
      "source": [
        "# Image (or Natural Scene) to Text"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zuaoi9H482Zl"
      },
      "source": [
        "! wget -q https://raw.githubusercontent.com/JohnSnowLabs/spark-ocr-workshop/master/jupyter/data/images/text_with_noise.png\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iR-lq_0j833B"
      },
      "source": [
        "image_df = spark.read.format(\"binaryFile\").load('text_with_noise.png').cache()\n",
        "\n",
        "# Read binary as image\n",
        "binary_to_image = BinaryToImage()\n",
        "binary_to_image.setInputCol(\"content\")\n",
        "binary_to_image.setOutputCol(\"image\")\n",
        "\n",
        "# Scale image\n",
        "scaler = ImageScaler()\n",
        "scaler.setInputCol(\"image\")\n",
        "scaler.setOutputCol(\"scaled_image\")\n",
        "scaler.setScaleFactor(2.0)\n",
        "\n",
        "# Binarize using adaptive tresholding\n",
        "binarizer = ImageAdaptiveThresholding()\n",
        "binarizer.setInputCol(\"scaled_image\")\n",
        "binarizer.setOutputCol(\"binarized_image\")\n",
        "binarizer.setBlockSize(71)\n",
        "binarizer.setOffset(65)\n",
        "\n",
        "remove_objects = ImageRemoveObjects()\n",
        "remove_objects.setInputCol(\"binarized_image\")\n",
        "remove_objects.setOutputCol(\"cleared_image\")\n",
        "remove_objects.setMinSizeObject(400)\n",
        "remove_objects.setMaxSizeObject(4000)\n",
        "\n",
        "# Run OCR\n",
        "ocr = ImageToText()\n",
        "ocr.setInputCol(\"cleared_image\")\n",
        "ocr.setOutputCol(\"text\")\n",
        "ocr.setConfidenceThreshold(50)\n",
        "ocr.setIgnoreResolution(False)\n",
        "\n",
        "# OCR pipeline\n",
        "noisy_pipeline = PipelineModel(stages=[\n",
        "    binary_to_image,\n",
        "    scaler,\n",
        "    binarizer,\n",
        "    remove_objects,\n",
        "    ocr\n",
        "])\n",
        "\n",
        "\n",
        "result = noisy_pipeline \\\n",
        ".transform(image_df) \\\n",
        ".cache()\n",
        "\n",
        "\n",
        "for r in result.distinct().collect():\n",
        "    print(\"Original: %s\" % r.path)\n",
        "    display_image(r.image)\n",
        "    print(\"Binarized\")\n",
        "    display_image(r.binarized_image)\n",
        "    print(\"Removing objects\")\n",
        "    display_image(r.cleared_image)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_SRXlRr9_eGk"
      },
      "source": [
        "print(\"\\n\".join([row.text for row in result.select(\"text\").collect()]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7VjHI0hjACLI"
      },
      "source": [
        "### Text from Scene"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q5BanjN2AlLK"
      },
      "source": [
        "!wget -q https://raw.githubusercontent.com/JohnSnowLabs/spark-nlp-workshop/master/tutorials/Certification_Trainings/Healthcare/data/ocr/natural_scene.jpeg"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xtu9hpEL9saX"
      },
      "source": [
        "image_df = spark.read.format(\"binaryFile\").load('natural_scene.jpeg').cache()\n",
        "\n",
        "# Apply morphology opening\n",
        "morpholy_operation = ImageMorphologyOperation()\n",
        "morpholy_operation.setKernelShape(KernelShape.DISK)\n",
        "morpholy_operation.setKernelSize(5)\n",
        "morpholy_operation.setOperation(\"closing\")\n",
        "morpholy_operation.setInputCol(\"cleared_image\")\n",
        "morpholy_operation.setOutputCol(\"corrected_image\")\n",
        "\n",
        "# Run OCR\n",
        "ocr = ImageToText()\n",
        "ocr.setInputCol(\"corrected_image\")\n",
        "ocr.setOutputCol(\"text\")\n",
        "ocr.setConfidenceThreshold(50)\n",
        "ocr.setIgnoreResolution(False)\n",
        "\n",
        "# OCR pipeline\n",
        "scene_pipeline = PipelineModel(stages=[\n",
        "    binary_to_image,\n",
        "    scaler,\n",
        "    binarizer,\n",
        "    remove_objects,\n",
        "    morpholy_operation,\n",
        "    ocr\n",
        "])\n",
        "\n",
        "result = scene_pipeline \\\n",
        ".transform(image_df) \\\n",
        ".cache()\n",
        "\n",
        "\n",
        "for r in result.distinct().collect():\n",
        "    print(\"Original: %s\" % r.path)\n",
        "    display_image(r.image)\n",
        "    print(\"Binarized\")\n",
        "    display_image(r.binarized_image)\n",
        "    print(\"Removing objects\")\n",
        "    display_image(r.cleared_image)\n",
        "    print(\"Morphology closing\")\n",
        "    display_image(r.corrected_image)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "58-RfPri0xk3"
      },
      "source": [
        "# DOCX Processing (version 1.10.0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G9oV5wtD8O-8"
      },
      "source": [
        "#### Read DOCX document as binary file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kB9dAvMk8Rl3"
      },
      "source": [
        "import pkg_resources\n",
        "doc_example = pkg_resources.resource_filename('sparkocr', 'resources/ocr/docs/doc2.docx')\n",
        "doc_example_df = spark.read.format(\"binaryFile\").load(doc_example).cache()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LjfPNSar1BsW"
      },
      "source": [
        "## DocxtoText"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TZPHmLAH7b9o"
      },
      "source": [
        "#### Extract text using DocToText transformer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xxRSsOVe0udx"
      },
      "source": [
        "from sparkocr.transformers import *\n",
        "\n",
        "doc_to_text = DocToText()\n",
        "doc_to_text.setInputCol(\"content\")\n",
        "doc_to_text.setOutputCol(\"text\")\n",
        "\n",
        "result = doc_to_text.transform(doc_example_df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QMA16ZGt7hGb"
      },
      "source": [
        "#### Display result DataFrame"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vqklj_AV7Hml"
      },
      "source": [
        "result.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ETclaT1h7mNT"
      },
      "source": [
        "#### Display extracted text"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rl3SfEUM7Rs-"
      },
      "source": [
        "print(\"\\n\".join([row.text for row in result.select(\"text\").collect()]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u0VlfU6w2AGI"
      },
      "source": [
        "## DocxToTextTable\n",
        "#### (Extracting table data from Microsoft DOCX documents)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XDazuQyP8mEb"
      },
      "source": [
        "#### Preview document using DocToPdf and PdfToImage transformers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JVfMqUkk0wkE"
      },
      "source": [
        "image_df = PdfToImage().transform(DocToPdf().setOutputCol(\"content\").transform(doc_example_df))\n",
        "for r in image_df.select(\"image\").collect():\n",
        "    display_image(r.image)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6J_p9fBQ9gpJ"
      },
      "source": [
        "#### Extract text using DocToText transformer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YNgJGNDr8-M3"
      },
      "source": [
        "doc_to_table = DocToTextTable()\n",
        "doc_to_table.setInputCol(\"content\")\n",
        "doc_to_table.setOutputCol(\"tables\")\n",
        "\n",
        "result = doc_to_table.transform(doc_example_df)\n",
        "\n",
        "result.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tbf6EQW99xhp"
      },
      "source": [
        "result.select(result[\"tables.chunks\"].getItem(3)[\"chunkText\"]).show(truncate=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9x5XXSAT-YxT"
      },
      "source": [
        "#### Display extracted data in JSON format"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ejlbb80W9w6B"
      },
      "source": [
        "import json\n",
        "df_json = result.select(\"tables\").toJSON()\n",
        "for row in df_json.collect():\n",
        "    print(json.dumps(json.loads(row), indent=4))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RVrFH1DU2DMN"
      },
      "source": [
        "# Text to Pdf"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z09Vuxus2Ccb"
      },
      "source": [
        "def pipeline():\n",
        "    # Transforrm PDF document to images per page\n",
        "    pdf_to_image = PdfToImage() \\\n",
        "        .setInputCol(\"content\") \\\n",
        "        .setOutputCol(\"image\") \\\n",
        "        .setKeepInput(True)\n",
        "    \n",
        "    # Run OCR\n",
        "    ocr = ImageToText() \\\n",
        "        .setInputCol(\"image\") \\\n",
        "        .setOutputCol(\"text\") \\\n",
        "        .setConfidenceThreshold(60) \\\n",
        "        .setIgnoreResolution(False) \\\n",
        "        .setPageSegMode(PageSegmentationMode.SPARSE_TEXT)\n",
        "    \n",
        "    # Render results to PDF\n",
        "    textToPdf = TextToPdf() \\\n",
        "        .setInputCol(\"positions\") \\\n",
        "        .setInputImage(\"image\") \\\n",
        "        .setOutputCol(\"pdf\")\n",
        "\n",
        "    pipeline = PipelineModel(stages=[\n",
        "        pdf_to_image,\n",
        "        ocr,\n",
        "        textToPdf\n",
        "    ])\n",
        "    \n",
        "    return pipeline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A_I2hWUy2IMI"
      },
      "source": [
        "!wget -q https://raw.githubusercontent.com/JohnSnowLabs/spark-nlp-workshop/master/tutorials/Certification_Trainings/Healthcare/data/ocr/test_document.pdf"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G81aUZ3q2OVh"
      },
      "source": [
        "pdf_example_df = spark.read.format(\"binaryFile\").load('test_document.pdf').cache()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aKDWZc9h27v0"
      },
      "source": [
        "result = pipeline().transform(pdf_example_df).cache()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_zeO9SVP32i-"
      },
      "source": [
        "result.columns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OyJVkb523iYV"
      },
      "source": [
        "display_image(PdfToImage().transform(pdf_example_df).select(\"image\").collect()[0].image)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vFSSheKJ29y7"
      },
      "source": [
        "# Store results to pdf file\n",
        "pdf = result.select(\"pdf\").head().pdf\n",
        "\n",
        "pdfFile = open(\"result.pdf\", \"wb\")\n",
        "\n",
        "pdfFile.write(pdf)\n",
        "\n",
        "pdfFile.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-RACahLy3Mf1"
      },
      "source": [
        "# Convert pdf to image and display¶\n",
        "\n",
        "image_df = PdfToImage() \\\n",
        "    .setInputCol(\"pdf\") \\\n",
        "    .setOutputCol(\"image\") \\\n",
        "    .transform(result.select(\"pdf\", \"path\"))\n",
        "\n",
        "for r in image_df.collect():\n",
        "    display_image(r.image)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SfzwFIw3aU0q"
      },
      "source": [
        "# Dicom Image Deidentifier "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RRlppvfyaYR-"
      },
      "source": [
        "## Deidentification Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OUzwlZ4NuWXm"
      },
      "source": [
        "from pyspark.ml import Pipeline\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "from sparknlp.annotator import *\n",
        "from sparknlp_jsl.annotator import *\n",
        "from sparknlp.base import *\n",
        "import sparknlp_jsl"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eUxjTdHLaesH"
      },
      "source": [
        "def deidentification_nlp_pipeline(input_column, prefix = \"\"):\n",
        "    document_assembler = DocumentAssembler() \\\n",
        "        .setInputCol(input_column) \\\n",
        "        .setOutputCol(prefix + \"document\")\n",
        "\n",
        "    # Sentence Detector annotator, processes various sentences per line\n",
        "    sentence_detector = SentenceDetector() \\\n",
        "        .setInputCols([prefix + \"document\"]) \\\n",
        "        .setOutputCol(prefix + \"sentence\")\n",
        "\n",
        "    tokenizer = Tokenizer() \\\n",
        "        .setInputCols([prefix + \"sentence\"]) \\\n",
        "        .setOutputCol(prefix + \"token\")\n",
        "\n",
        "    # Clinical word embeddings\n",
        "    word_embeddings = WordEmbeddingsModel.pretrained(\"embeddings_clinical\", \"en\", \"clinical/models\") \\\n",
        "        .setInputCols([prefix + \"sentence\", prefix + \"token\"]) \\\n",
        "        .setOutputCol(prefix + \"embeddings\")\n",
        "    # NER model trained on i2b2 (sampled from MIMIC) dataset\n",
        "    clinical_ner = MedicalNerModel.pretrained(\"ner_deid_large\", \"en\", \"clinical/models\") \\\n",
        "        .setInputCols([prefix + \"sentence\", prefix + \"token\", prefix + \"embeddings\"]) \\\n",
        "        .setOutputCol(prefix + \"ner\")\n",
        "\n",
        "    custom_ner_converter = NerConverter() \\\n",
        "        .setInputCols([prefix + \"sentence\", prefix + \"token\", prefix + \"ner\"]) \\\n",
        "        .setOutputCol(prefix + \"ner_chunk\") \\\n",
        "        .setWhiteList(['NAME', 'AGE', 'CONTACT', 'LOCATION', 'PROFESSION', 'PERSON', 'DATE'])\n",
        "\n",
        "    nlp_pipeline = Pipeline(stages=[\n",
        "            document_assembler,\n",
        "            sentence_detector,\n",
        "            tokenizer,\n",
        "            word_embeddings,\n",
        "            clinical_ner,\n",
        "            custom_ner_converter\n",
        "        ])\n",
        "    empty_data = spark.createDataFrame([[\"\"]]).toDF(input_column)\n",
        "    nlp_model = nlp_pipeline.fit(empty_data)\n",
        "    return nlp_model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2SgIcnWNafMZ"
      },
      "source": [
        "## Define OCR transformers and pipeline"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PSDZ8_qxaloZ"
      },
      "source": [
        "# Convert to images\n",
        "binary_to_image = BinaryToImage() \\\n",
        "    .setInputCol(\"content\") \\\n",
        "    .setOutputCol(\"image_raw\")\n",
        "\n",
        "# Extract text from image\n",
        "ocr = ImageToText() \\\n",
        "    .setInputCol(\"image_raw\") \\\n",
        "    .setOutputCol(\"text\") \\\n",
        "    .setIgnoreResolution(False) \\\n",
        "    .setPageIteratorLevel(PageIteratorLevel.SYMBOL) \\\n",
        "    .setPageSegMode(PageSegmentationMode.SPARSE_TEXT) \\\n",
        "    .setConfidenceThreshold(70)\n",
        "\n",
        "# Found coordinates of sensitive data\n",
        "position_finder = PositionFinder() \\\n",
        "    .setInputCols(\"ner_chunk\") \\\n",
        "    .setOutputCol(\"coordinates\") \\\n",
        "    .setPageMatrixCol(\"positions\") \\\n",
        "    .setMatchingWindow(1000) \\\n",
        "    .setPadding(1)\n",
        "\n",
        "# Draw filled rectangle for hide sensitive data\n",
        "drawRegions = ImageDrawRegions()  \\\n",
        "    .setInputCol(\"image_raw\")  \\\n",
        "    .setInputRegionsCol(\"coordinates\")  \\\n",
        "    .setOutputCol(\"image_with_regions\")  \\\n",
        "    .setFilledRect(True) \\\n",
        "    .setRectColor(Color.gray)\n",
        "    \n",
        "\n",
        "# OCR pipeline\n",
        "pipeline = Pipeline(stages=[\n",
        "    binary_to_image,\n",
        "    ocr,\n",
        "    deidentification_nlp_pipeline(input_column=\"text\"),\n",
        "    position_finder,\n",
        "    drawRegions\n",
        "])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LRhLoClhamOp"
      },
      "source": [
        "### Load Image"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lkKJLHuCapmx"
      },
      "source": [
        "import pkg_resources\n",
        "image_path = pkg_resources.resource_filename('sparkocr', 'resources/ocr/images/p1.jpg')\n",
        "image_df = spark.read.format(\"binaryFile\").load(image_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dpvI9j6JaqVG"
      },
      "source": [
        "### Run Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pgYxkmsGaudx"
      },
      "source": [
        "result = pipeline.fit(image_df).transform(image_df).cache()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AykT2toriiBA"
      },
      "source": [
        "result.select('text').show(1, truncate=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YLzDbCgZauJV"
      },
      "source": [
        "# Chunks to hide\n",
        "result.select('ner_chunk').show(2, False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B7Tnh7VxbGWP"
      },
      "source": [
        "# Coordinates of Chunks to Hide\n",
        "result.select('coordinates').show(2, False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mrRt2pnEau91"
      },
      "source": [
        "## Show original and deidentified images"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R8et5ogyq6Ce"
      },
      "source": [
        "from sparkocr.utils import display_image, to_pil_image\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c2y9RMzCa08H"
      },
      "source": [
        "for r in result.select(\"image_raw\", \"image_with_regions\").collect():\n",
        "    img_orig = r.image_raw\n",
        "    img_deid = r.image_with_regions\n",
        "\n",
        "    img_pil_orig = to_pil_image(img_orig, img_orig.mode)\n",
        "    img_pil_deid = to_pil_image(img_deid, img_deid.mode)\n",
        "\n",
        "    plt.figure(figsize=(24,16))\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.imshow(img_pil_orig, cmap='gray')\n",
        "    plt.title('original')\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.imshow(img_pil_deid, cmap='gray')\n",
        "    plt.title(\"de-id'd\")\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0sJIIvpx7k--"
      },
      "source": [
        "# Dicom to Image"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HHUrLvgS7Yp4"
      },
      "source": [
        "! mkdir dicom\n",
        "! wget -q https://raw.githubusercontent.com/JohnSnowLabs/spark-ocr-workshop/master/jupyter/data/dicom/deidentify-brains-front-medical-3.dcm -O /content/dicom/dicom_1.dcm\n",
        "! wget -q https://raw.githubusercontent.com/JohnSnowLabs/spark-ocr-workshop/master/jupyter/data/dicom/deidentify-medical-1.dcm  -O /content/dicom/dicom_2.dcm\n",
        "! wget -q https://raw.githubusercontent.com/JohnSnowLabs/spark-ocr-workshop/master/jupyter/data/dicom/deidentify-medical-2.dcm  -O /content/dicom/dicom_3.dcm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q-2bZs417Oxk"
      },
      "source": [
        "dicom_path = './dicom/*.dcm'\n",
        "\n",
        "# Read dicom file as binary file\n",
        "dicom_df = spark.read.format(\"binaryFile\").load(dicom_path)\n",
        "\n",
        "\n",
        "dicomToImage = DicomToImage() \\\n",
        "  .setInputCol(\"content\") \\\n",
        "  .setOutputCol(\"image\") \\\n",
        "  .setMetadataCol(\"meta\")\n",
        "\n",
        "data = dicomToImage.transform(dicom_df)\n",
        "\n",
        "for image in data.collect():\n",
        "      display_image(image.image)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i-_aoUYc8x-Q"
      },
      "source": [
        "# Extract text from image\n",
        "ocr = ImageToText() \\\n",
        "    .setInputCol(\"image\") \\\n",
        "    .setOutputCol(\"text\") \\\n",
        "    .setIgnoreResolution(False) \\\n",
        "    .setOcrParams([\"preserve_interword_spaces=0\"])\n",
        "\n",
        "\n",
        "print(\"\\n\".join([row.text for row in ocr.transform(data).select(\"text\").collect()]))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mWLe4J59uTO-"
      },
      "source": [
        "# More example here\n",
        "\n",
        "https://github.com/JohnSnowLabs/spark-ocr-workshop"
      ]
    }
  ]
}